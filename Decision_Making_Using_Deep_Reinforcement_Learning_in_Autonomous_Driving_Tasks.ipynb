{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Install and import libraries**"
      ],
      "metadata": {
        "id": "M5xVVn6WZpyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "TgEQKew6kHaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gym==0.26.2\n",
        "!pip install -q highway-env\n",
        "!pip install -q moviepy --upgrade"
      ],
      "metadata": {
        "id": "RnIHFZ-Ebv9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgWohBwPZOhY"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import os\n",
        "import base64, io\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from itertools import count\n",
        "from PIL import Image\n",
        "from collections import deque, namedtuple\n",
        "from scipy.stats import norm , t\n",
        "from scipy.stats import ttest_ind\n",
        "from tqdm.notebook import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "# Pytorch librearies\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "import torch.optim as optim\n",
        "# For visualization\n",
        "from gym.wrappers.monitoring import video_recorder\n",
        "from IPython.display import HTML\n",
        "from IPython import display\n",
        "plt.style.use('ggplot')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Define Class and Hyperparameters**"
      ],
      "metadata": {
        "id": "b9h17P1WZ5bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameters"
      ],
      "metadata": {
        "id": "MAzf2DwdiLQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
        "BATCH_SIZE = 64         # minibatch size\n",
        "GAMMA = 0.99            # discount factor\n",
        "ð›¼ = 1e-3                # for soft update of target parameters\n",
        "LR = 5e-4               # learning rate\n",
        "UPDATE_EVERY = 4        # how often to update the network"
      ],
      "metadata": {
        "id": "5H2xtOGXaFVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_HW5 = '/content/drive/MyDrive/RL/HW/HW5/'"
      ],
      "metadata": {
        "id": "07GdggWw5ZlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device.type"
      ],
      "metadata": {
        "id": "G2Php3MJj-Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Class Neural Network Architecture"
      ],
      "metadata": {
        "id": "lNgk2ltIgr1a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Class Linear"
      ],
      "metadata": {
        "id": "H6d5lN1xg4sZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QNetwork_Linear(nn.Module):\n",
        "\n",
        "    def __init__(self, state_size, action_size, seed):\n",
        "        super(QNetwork_Linear, self).__init__()\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "        self.fc1 = nn.Linear(state_size, 125)\n",
        "        self.fc2 = nn.Linear(125, 125)\n",
        "        self.fc3 = nn.Linear(125, action_size)\n",
        "\n",
        "    def forward(self, state):\n",
        "        # Build a network that maps state -> action values\n",
        "        x = self.fc1(state)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "2YcITgYGggDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Class CNN"
      ],
      "metadata": {
        "id": "QmIuSaeNhWD7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QNetwork_CNN(nn.Module):\n",
        "\n",
        "    def __init__(self, action_size, seed):\n",
        "        super(QNetwork_CNN, self).__init__()\n",
        "        self.seed = torch.manual_seed(seed)\n",
        "        self.conv1 = nn.Conv2d(3, 128, kernel_size=3)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=2)\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.conv2 = nn.Conv2d(128, 128, kernel_size=3)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(38912,64)\n",
        "        self.fc2 = nn.Linear(64, action_size)\n",
        "\n",
        "    def forward(self, state):\n",
        "        # Build a network that maps state -> action values\n",
        "        x = self.dropout(self.maxpool(self.relu(self.conv1(state))))\n",
        "        x = self.dropout(self.maxpool(self.relu(self.conv2(x))))\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Aa-42Q16hVGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Class Replay Buffer"
      ],
      "metadata": {
        "id": "Msl-H1WHkugm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer():\n",
        "\n",
        "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=buffer_size)\n",
        "        self.batch_size = batch_size\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "        self.seed = random.seed(seed)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        # Add a new experience to memory\n",
        "        e = self.experience(state, action, reward, next_state, done)\n",
        "        self.memory.append(e)\n",
        "\n",
        "    def sample(self):\n",
        "        # Randomly sample a batch of experiences from memory\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "\n",
        "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
        "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
        "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
        "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
        "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
        "\n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the current size of internal memory\n",
        "        return len(self.memory)"
      ],
      "metadata": {
        "id": "7zFfQfnbktYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Class Agent"
      ],
      "metadata": {
        "id": "OG5D5Dd5jnJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent():\n",
        "\n",
        "    def __init__(self, state_size, action_size, network_type, seed):\n",
        "\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.seed = random.seed(seed)\n",
        "\n",
        "        self.network_type = network_type\n",
        "        # Q-Network\n",
        "        if network_type=='linear' or network_type=='Linear':\n",
        "            self.qnetwork_local = QNetwork_Linear(state_size, action_size, seed).to(device)\n",
        "            self.qnetwork_target = QNetwork_Linear(state_size, action_size, seed).to(device)\n",
        "        elif network_type=='cnn' or network_type=='CNN':\n",
        "            self.qnetwork_local = QNetwork_CNN(action_size, seed).to(device)\n",
        "            self.qnetwork_target = QNetwork_CNN(action_size, seed).to(device)\n",
        "\n",
        "        # Optimizer\n",
        "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
        "\n",
        "        # Replay memory\n",
        "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
        "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
        "        self.t_step = 0\n",
        "\n",
        "        #resize image\n",
        "        self.resize = T.Compose([T.ToPILImage(), T.Resize(40, interpolation=Image.CUBIC), T.ToTensor()])\n",
        "\n",
        "    def get_screen(self, screen):\n",
        "        screen = np.ascontiguousarray(screen, dtype=np.float32) / 255\n",
        "        screen = torch.from_numpy(screen)\n",
        "        screen = screen.permute(2,0,1)\n",
        "        screen_resized = self.resize(screen).unsqueeze(0)\n",
        "        return screen_resized\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "        # Save experience in replay memory\n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "\n",
        "        # Learn every UPDATE_EVERY time steps.\n",
        "        self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
        "        if self.t_step == 0:\n",
        "            # If enough samples are available in memory, get random subset and learn\n",
        "            if len(self.memory) > BATCH_SIZE:\n",
        "                experiences = self.memory.sample()\n",
        "                self.learn(experiences, GAMMA)\n",
        "\n",
        "    def act(self, state, eps=0):\n",
        "        if self.network_type=='linear' or self.network_type=='Linear':\n",
        "            state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        elif self.network_type=='cnn' or self.network_type=='CNN':\n",
        "            state = state.to(device)\n",
        "        self.qnetwork_local.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            action_values = self.qnetwork_local(state)\n",
        "        self.qnetwork_local.train()\n",
        "\n",
        "        # Epsilon-greedy action selection\n",
        "        if random.random() > eps:\n",
        "            return np.argmax(action_values.cpu().data.numpy())\n",
        "        else:\n",
        "            return random.choice(np.arange(self.action_size))\n",
        "\n",
        "    def learn(self, experiences, gamma):\n",
        "        #Update value parameters using given batch of experience tuples\n",
        "\n",
        "        ## Obtain random minibatch of tuples from D\n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "\n",
        "        ## Compute and minimize the loss\n",
        "        ### Extract next maximum estimated value from target network\n",
        "        q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
        "        ### Calculate target value from bellman equation\n",
        "        q_targets = rewards + gamma * q_targets_next * (1 - dones)\n",
        "        ### Calculate expected value from local network\n",
        "        q_expected = self.qnetwork_local(states).gather(1, actions)\n",
        "\n",
        "        ### Loss calculation (we used Mean squared error)\n",
        "        loss = F.mse_loss(q_expected, q_targets)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # ------------------- update target network ------------------- #\n",
        "        self.soft_update(self.qnetwork_local, self.qnetwork_target, ð›¼)\n",
        "\n",
        "    def soft_update(self, local_model, target_model, ð›¼):\n",
        "        # Soft update model parameters. Î¸_target = ð›¼*Î¸_local + (1-ð›¼)*Î¸_target\n",
        "        ## local_model (PyTorch model): weights will be copied from\n",
        "        ## target_model (PyTorch model): weights will be copied to\n",
        "        ## ð›¼ (float): interpolation parameter\n",
        "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
        "            target_param.data.copy_(ð›¼*local_param.data + (1-ð›¼)*target_param.data)"
      ],
      "metadata": {
        "id": "ebZqPgWljgQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Class DQN"
      ],
      "metadata": {
        "id": "wsdHZdO8q3Zd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN():\n",
        "    def __init__(self, env, env_name, model_path, data_path, network_type, env_name_source=None, model_path_source=None, transfer_episode=None):\n",
        "        self.env = env\n",
        "        self.env_name = env_name\n",
        "        self.model_path = model_path\n",
        "        self.data_path = data_path\n",
        "        self.network_type = network_type\n",
        "        self.env_name_source = env_name_source\n",
        "        self.model_path_source = model_path_source\n",
        "        self.transfer_episode = transfer_episode\n",
        "        self.df_reward = pd.DataFrame()\n",
        "        self.seed = [0, 5, 11, 123, 487]\n",
        "        self.env.config['policy_frequency'] = env.config['policy_frequency']\n",
        "\n",
        "    # If we have state\n",
        "    def train_with_state(self, n_iteration, n_training_episodes, max_step, eps_start=1.0, eps_end=0.001, eps_decay=0.995):\n",
        "        print(\"Starting to train agent with state ...\")\n",
        "        if n_iteration[0] != 1:\n",
        "            self.df_reward = pd.read_excel(self.data_path+'Episodes_reward_run_{}.xlsx'.format(n_iteration[0]-1))\n",
        "\n",
        "        for iter in tqdm(n_iteration):\n",
        "            episodes_reward = []\n",
        "            # Define agent\n",
        "            # seed for initial random weights of network\n",
        "            state_size = int(np.prod(self.env.observation_space.shape))\n",
        "            action_size = self.env.action_space.n\n",
        "            agent = Agent(state_size=state_size, action_size=action_size, network_type=self.network_type, seed=self.seed[iter-1])\n",
        "            if (self.env_name_source != None) and (self.model_path_source != None) and (self.transfer_episode != None):\n",
        "                agent.qnetwork_local.load_state_dict(torch.load(self.model_path_source + 'Models_Run_{}/checkpoint_dqn_state_{}_local_{}.pth'\\\n",
        "                                                                                  .format(iter, self.env_name_source, self.transfer_episode)))\n",
        "                agent.qnetwork_target.load_state_dict(torch.load(self.model_path_source + 'Models_Run_{}/checkpoint_dqn_state_{}_target_{}.pth'\\\n",
        "                                                                                  .format(iter, self.env_name_source, self.transfer_episode)))\n",
        "            # initialize epsilon\n",
        "            eps = eps_start\n",
        "\n",
        "            for episode_num in tqdm(range(1, n_training_episodes+1)):\n",
        "                # decrease epsilon\n",
        "                eps = max(eps_end, eps_decay*eps)\n",
        "                # get initial state\n",
        "                state = self.env.reset()[0]\n",
        "\n",
        "                done = False\n",
        "                total_rewards_episode = 0\n",
        "                # repeat\n",
        "                for step in range(max_step):\n",
        "                    # Choose action\n",
        "                    action = agent.act(state.ravel(), eps)\n",
        "                    # Go to next state and get reward\n",
        "                    next_state, reward, done, _, _ = self.env.step(action)\n",
        "                    total_rewards_episode += reward\n",
        "                    # AGENT STEP FUNCTION\n",
        "                    agent.step(state.ravel(), action, reward, next_state.ravel(), done)\n",
        "\n",
        "                    if done:\n",
        "                        break\n",
        "                    # Updating State\n",
        "                    state = next_state\n",
        "                episodes_reward.append(total_rewards_episode)\n",
        "                # Printing ...\n",
        "                print('\\rEpisode {}\\tAverage reward: {:.2f}\\tEpsilon: {:.3f}'.format(episode_num, np.mean(episodes_reward), eps), end='')\n",
        "                if episode_num % 100 == 0:\n",
        "                    print('\\rEpisode {}\\tAverage reward: {:.2f}\\tEpsilon: {:.3f}'.format(episode_num, np.mean(episodes_reward), eps))\n",
        "                if episode_num % 200 == 0:\n",
        "                    torch.save(agent.qnetwork_local.state_dict(), self.model_path + 'Models_Run_{}/checkpoint_dqn_state_{}_local_{}.pth'\\\n",
        "                                                                                                .format(iter, self.env_name, episode_num))\n",
        "                    torch.save(agent.qnetwork_target.state_dict(), self.model_path + 'Models_Run_{}/checkpoint_dqn_state_{}_target_{}.pth'\\\n",
        "                                                                                                .format(iter, self.env_name, episode_num))\n",
        "            # list of rewards with n_interations * n_episodes dimention\n",
        "            self.df_reward['Run_{}'.format(iter)] = episodes_reward\n",
        "            self.df_reward.to_excel(self.data_path+'Episodes_reward_run_{}.xlsx'.format(iter), index=False)\n",
        "        return self.df_reward\n",
        "\n",
        "    # If we have observation\n",
        "    def train_with_observation(self, n_iteration, n_training_episodes, max_step, eps_start=1.0, eps_end=0.001, eps_decay=0.995):\n",
        "        print(\"Starting to train agent with observation ...\")\n",
        "        if n_iteration[0] != 1:\n",
        "            self.df_reward = pd.read_excel(self.data_path+'Episodes_reward_run_{}.xlsx'.format(n_iteration[0]-1))\n",
        "\n",
        "        for iter in tqdm(n_iteration):\n",
        "            episodes_reward = []\n",
        "            # Define agent\n",
        "            # seed for initial random weights of network\n",
        "            state_size = int(np.prod(self.env.observation_space.shape))\n",
        "            action_size = self.env.action_space.n\n",
        "            agent = Agent(state_size=state_size, action_size=action_size, network_type=self.network_type, seed=self.seed[iter-1])\n",
        "            if (self.env_name_source != None) and (self.model_path_source != None) and (self.transfer_episode != None):\n",
        "                agent.qnetwork_local.load_state_dict(torch.load(self.model_path_source + 'Models_Run_{}/checkpoint_dqn_observation_{}_local_{}.pth'\\\n",
        "                                                                                          .format(iter, self.env_name_source, self.transfer_episode)))\n",
        "                agent.qnetwork_target.load_state_dict(torch.load(self.model_path_source + 'Models_Run_{}/checkpoint_dqn_observation_{}_target_{}.pth'\\\n",
        "                                                                                          .format(iter, self.env_name_source, self.transfer_episode)))\n",
        "            # initialize epsilon\n",
        "            eps = eps_start\n",
        "\n",
        "            for episode_num in tqdm(range(1, n_training_episodes+1)):\n",
        "                # decrease epsilon\n",
        "                eps = max(eps_end, eps_decay*eps)\n",
        "                # get initial observation\n",
        "                self.env.reset()\n",
        "                last_screen = agent.get_screen(self.env.render())\n",
        "                current_screen = agent.get_screen(self.env.render())\n",
        "                observation = current_screen - last_screen\n",
        "\n",
        "                done = False\n",
        "                total_rewards_episode = 0\n",
        "                # repeat\n",
        "                for step in range(max_step):\n",
        "                    # Choose action\n",
        "                    action = agent.act(observation, eps)\n",
        "                    # Go to next observation and get reward\n",
        "                    _, reward, done, _, _ = self.env.step(action)\n",
        "                    total_rewards_episode += reward\n",
        "                    # get next observation\n",
        "                    last_screen = current_screen\n",
        "                    current_screen = agent.get_screen(self.env.render())\n",
        "                    next_observation = current_screen - last_screen\n",
        "                    # AGENT STEP FUNCTION\n",
        "                    agent.step(observation, action, reward, next_observation, done)\n",
        "\n",
        "                    if done:\n",
        "                        break\n",
        "                    # Updating Observation\n",
        "                    observation = next_observation\n",
        "                episodes_reward.append(total_rewards_episode)\n",
        "                # Printing ...\n",
        "                print('\\rEpisode {}\\tAverage reward: {:.2f}\\tEpsilon: {:.3f}'.format(episode_num, np.mean(episodes_reward), eps), end='')\n",
        "                if episode_num % 100 == 0:\n",
        "                    print('\\rEpisode {}\\tAverage reward: {:.2f}\\tEpsilon: {:.3f}'.format(episode_num, np.mean(episodes_reward), eps))\n",
        "                if episode_num % 200 == 0:\n",
        "                    torch.save(agent.qnetwork_local.state_dict(), self.model_path + 'Models_Run_{}/checkpoint_dqn_observation_{}_local_{}.pth'\\\n",
        "                                                                                                .format(iter, self.env_name, episode_num))\n",
        "                    torch.save(agent.qnetwork_target.state_dict(), self.model_path + 'Models_Run_{}/checkpoint_dqn_observation_{}_target_{}.pth'\\\n",
        "                                                                                                .format(iter, self.env_name, episode_num))\n",
        "            # list of rewards with n_interations * n_episodes dimention\n",
        "            self.df_reward['Run_{}'.format(iter)] = episodes_reward\n",
        "            self.df_reward.to_excel(self.data_path+'Episodes_reward_run_{}.xlsx'.format(iter), index=False)\n",
        "        return self.df_reward\n",
        "\n",
        "    def evaluation(self, video_path, evaluate_type, iter_num, evaluate_episode_num, use_saved_model=True):\n",
        "        #######################################################################################\n",
        "        # This part is for solving the 'error: No available video device' and 'error: x11 not avaliable'\n",
        "        os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
        "        env1 = gym.make(self.env_name, render_mode='rgb_array')\n",
        "        env1.reset()\n",
        "        env1.render()\n",
        "        os.environ['SDL_VIDEODRIVER'] = \"x11\"\n",
        "        env2 = gym.make(self.env_name, render_mode='rgb_array')\n",
        "        env2.reset()\n",
        "        env2.render()\n",
        "        #######################################################################################\n",
        "        vid = video_recorder.VideoRecorder(self.env, path=video_path)\n",
        "\n",
        "        state_size = int(np.prod(self.env.observation_space.shape))\n",
        "        action_size = self.env.action_space.n\n",
        "        agent = Agent(state_size=state_size, action_size=action_size, network_type=self.network_type, seed=11)#colision 1, 2, 4, 5 for highwqay  for merge 11\n",
        "\n",
        "        if use_saved_model:\n",
        "            agent.qnetwork_local.load_state_dict(torch.load(self.model_path + 'Models_Run_{}/checkpoint_dqn_{}_{}_local_{}.pth'\\\n",
        "                                .format(iter_num, evaluate_type, self.env_name, evaluate_episode_num), map_location=torch.device(device.type)))\n",
        "            agent.qnetwork_target.load_state_dict(torch.load(self.model_path + 'Models_Run_{}/checkpoint_dqn_{}_{}_target_{}.pth'\\\n",
        "                                .format(iter_num, evaluate_type, self.env_name, evaluate_episode_num), map_location=torch.device(device.type)))\n",
        "\n",
        "        if evaluate_type=='state':\n",
        "            state = self.env.reset()[0]\n",
        "        elif evaluate_type=='observation':\n",
        "            self.env.reset()\n",
        "            last_screen = agent.get_screen(self.env.render())\n",
        "            current_screen = agent.get_screen(self.env.render())\n",
        "            state = current_screen - last_screen\n",
        "\n",
        "        done = False\n",
        "        step = 0\n",
        "        sum_rewards = 0\n",
        "        while not done:\n",
        "            print('\\rstep:{}'.format(step), end=\"\")\n",
        "\n",
        "            frame = self.env.render()\n",
        "            vid.capture_frame()\n",
        "\n",
        "            if evaluate_type=='state':\n",
        "                action = agent.act(state.ravel())\n",
        "                state, reward, done, _, _ = self.env.step(action)\n",
        "\n",
        "            elif evaluate_type=='observation':\n",
        "                action = agent.act(state)\n",
        "                _, reward, done, _, _ = self.env.step(action)\n",
        "                last_screen = current_screen\n",
        "                current_screen = agent.get_screen(self.env.render())\n",
        "                state = current_screen - last_screen\n",
        "\n",
        "            sum_rewards += reward\n",
        "            step+=1\n",
        "        print()\n",
        "        self.env.close()\n",
        "        return sum_rewards"
      ],
      "metadata": {
        "id": "BDvtV6_XrCgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Class Show and Plot"
      ],
      "metadata": {
        "id": "Szuss2N-fdve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class show_and_plot():\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def show_video(self, directory, file_name):\n",
        "        mp4list = glob.glob(directory+'*.mp4')\n",
        "        if len(mp4list) > 0:\n",
        "            mp4 = directory+file_name\n",
        "            video = io.open(mp4, 'r+b').read()\n",
        "            encoded = base64.b64encode(video)\n",
        "            display.display(HTML(data='''<video alt=\"test\" autoplay\n",
        "                    loop controls style=\"height: 165px; width: 650px;\">\n",
        "                    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "                </video>'''.format(encoded.decode('ascii'))))\n",
        "        else:\n",
        "            print(\"Could not find video\")\n",
        "\n",
        "    def plot_training_result(self, episodes_rewards, ax, title, label, window=20, stride=5, alpha=0.3):\n",
        "\n",
        "        n_iter, n_episode = episodes_rewards.shape\n",
        "\n",
        "        smooth_reward = []\n",
        "\n",
        "        for i in range(int(np.round((n_episode-window)/stride))+1):\n",
        "            k = np.sum(episodes_rewards[:,i*stride:i*stride+window], axis=1) / window\n",
        "            smooth_reward.append(k)\n",
        "\n",
        "        smooth_reward = np.array(smooth_reward).T\n",
        "\n",
        "        _ , n_smooth = smooth_reward.shape\n",
        "\n",
        "        X =  range(1, stride*n_smooth+1, stride)\n",
        "\n",
        "        x_bar = np.mean(smooth_reward, axis=0)\n",
        "        sigma = np.std(smooth_reward, axis=0)\n",
        "        SE = sigma / np.sqrt(n_iter)\n",
        "        interval =  1 - alpha\n",
        "        if n_iter < 30:\n",
        "            t_statistic = t.interval(interval, df = n_iter - 1)[1]\n",
        "            ME = t_statistic * SE\n",
        "        else:\n",
        "            z_statistic = norm.interval(interval)[1]\n",
        "            ME = z_statistic * SE\n",
        "\n",
        "\n",
        "        ax.plot(X, x_bar, label=label)\n",
        "        if n_iter >= 2:\n",
        "            ax.fill_between(X, (x_bar - ME), (x_bar + ME), alpha=alpha)\n",
        "        ax.set_xlabel(\"Episodes\", fontsize=13)\n",
        "        ax.set_ylabel(\"Average reward\", fontsize=13)\n",
        "        ax.set_title(title + \"\\nwindow size={}, stride={}\".format(window, stride), fontsize=15)\n",
        "        return ax"
      ],
      "metadata": {
        "id": "ngWJWiBGfYQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 1**"
      ],
      "metadata": {
        "id": "zDMTJgSA_ZAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.style.use('default')\n",
        "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
        "env1 = gym.make('highway-v0', render_mode='rgb_array')\n",
        "env1.reset()\n",
        "env1.render()\n",
        "os.environ['SDL_VIDEODRIVER'] = \"x11\"\n",
        "env2 = gym.make('highway-v0', render_mode='rgb_array')\n",
        "env2.reset()\n",
        "plt.imshow(env2.render());\n",
        "plt.style.use('ggplot')"
      ],
      "metadata": {
        "id": "LPQhILCH_ZY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Number of actions:', env2.action_space.n)\n",
        "print('\\nShape of states:', env2.observation_space.shape)\n",
        "print('\\nA sample of states:\\n', env2.observation_space.sample())"
      ],
      "metadata": {
        "id": "oX0CoDguA68q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env2.config"
      ],
      "metadata": {
        "id": "jV1p_48kCMsj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 2**"
      ],
      "metadata": {
        "id": "xjjPZTfNZgYH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## merge-v0"
      ],
      "metadata": {
        "id": "-mfBz2fwaeo7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = path_HW5 + 'Videos/'\n",
        "model_path = path_HW5 + 'Models/1_Models_merge/'\n",
        "data_path_1 = path_HW5 + 'Data_Average_Reward/1_Data_merge/'\n",
        "env_name = 'merge-v0'\n",
        "env = gym.make(env_name, render_mode='rgb_array')\n",
        "dqn_merge = DQN(env, env_name, model_path, data_path_1, network_type='linear')"
      ],
      "metadata": {
        "id": "w3A4mz3FbBNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "aTtOkW8ja8Z4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_reward_dqn_merge = dqn_merge.train_with_state(n_iteration=[1, 2, 3, 4, 5], n_training_episodes=3600, max_step=10000)"
      ],
      "metadata": {
        "id": "pxQMPkGJbHVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating"
      ],
      "metadata": {
        "id": "yGhUfb_LatNK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dqn_merge.env.config['policy_frequency'] = 10\n",
        "sum_rewards = dqn_merge.evaluation(video_path+\"merge.mp4\", evaluate_type='state', iter_num=2, evaluate_episode_num=2000, use_saved_model=True)\n",
        "print('\\n\\nSum rewards in one EPISODE:', sum_rewards)"
      ],
      "metadata": {
        "id": "qbQrFdoYazyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_and_plot().show_video(directory=video_path, file_name='merge.mp4')"
      ],
      "metadata": {
        "id": "fRrq13SFf0Sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Plotting**"
      ],
      "metadata": {
        "id": "4oX9P3ASayfv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_reward_dqn_merge = pd.read_excel(data_path_1 + 'Episodes_reward_run_5.xlsx')\n",
        "episodes_rewards_merge = df_reward_dqn_merge.T.to_numpy()\n",
        "\n",
        "fig, ax = plt.subplots(figsize = (12, 7.5))\n",
        "ax = show_and_plot().plot_training_result(episodes_rewards_merge, ax, title='DQN', label='Merge task with state',\n",
        "                                                                                                          window=50, stride=10, alpha=0.3)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "fig.savefig(path_HW5+'Images/1-Merge-task-state.png', dpi=300)"
      ],
      "metadata": {
        "id": "NKp-Y1mbS3sC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 3**"
      ],
      "metadata": {
        "id": "yzZxb7aLZjoY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## highway-fast-v0"
      ],
      "metadata": {
        "id": "Ty03IwTwbY_x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = path_HW5 + 'Videos/'\n",
        "model_path = path_HW5 + 'Models/2_Models_fastHighway/'\n",
        "data_path_2 = path_HW5 + 'Data_Average_Reward/2_Data_fastHighway/'\n",
        "env_name = 'highway-fast-v0'\n",
        "env = gym.make(env_name, render_mode='rgb_array')\n",
        "dqn_fastHighway = DQN(env, env_name, model_path, data_path_2, network_type='linear')"
      ],
      "metadata": {
        "id": "rXgxKO4bbY_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "cz-nLJ72bY_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_reward_dqn_fastHighway = dqn_fastHighway.train_with_state(n_iteration=[1, 2, 3, 4, 5], n_training_episodes=3600, max_step=10000)"
      ],
      "metadata": {
        "id": "MsjmCSvPbY_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_reward_dqn_fastHighway = dqn_fastHighway.train_with_state(n_iteration=[4, 5], n_training_episodes=3600, max_step=10000)"
      ],
      "metadata": {
        "id": "6IK3mZAHtTxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating"
      ],
      "metadata": {
        "id": "QO6v2xoqbY_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dqn_fastHighway.env.config['policy_frequency'] = 5\n",
        "sum_rewards = dqn_fastHighway.evaluation(video_path+\"fastHighway.mp4\", evaluate_type='state', iter_num=1, evaluate_episode_num=3400, use_saved_model=True)\n",
        "print('\\n\\nSum rewards in one EPISODE:', sum_rewards)"
      ],
      "metadata": {
        "id": "uLxTUnk0kLPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_and_plot().show_video(directory=video_path, file_name='fastHighway.mp4')"
      ],
      "metadata": {
        "id": "k_s_n_i7kPVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transfer learning from merge-v0 on highway-fast-v0"
      ],
      "metadata": {
        "id": "zYcVu3v-cGog"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = path_HW5 + 'Videos/'\n",
        "model_path_source = path_HW5 + 'Models/1_Models_merge/'\n",
        "model_path_destination = path_HW5 + 'Models/3_Models_merge_transfer_learning_to_fastHighway/'\n",
        "data_path_3 = path_HW5 + 'Data_Average_Reward/3_Data_merge_transfer_learning_to_fastHighway/'\n",
        "env_name_source = 'merge-v0'\n",
        "env_name_destination = 'highway-fast-v0'\n",
        "env = gym.make(env_name_destination, render_mode='rgb_array')\n",
        "dqn_fastHighway_transferred = DQN(env, env_name_destination, model_path_destination, data_path_3, network_type='linear',\n",
        "                                          env_name_source=env_name_source, model_path_source=model_path_source, transfer_episode=3600)"
      ],
      "metadata": {
        "id": "z7OYHNFOcGog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "6O-HIYXxcGoh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_reward_dqn_fastHighway_transferred = dqn_fastHighway_transferred.train_with_state(n_iteration=[1, 2, 3, 4, 5],\n",
        "                                                                                                      n_training_episodes=3600, max_step=10000)"
      ],
      "metadata": {
        "id": "3LpnyUbkcGoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating"
      ],
      "metadata": {
        "id": "wWYuZxNscGoh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dqn_fastHighway_transferred.env.config['policy_frequency'] = 5\n",
        "sum_rewards = dqn_fastHighway_transferred.evaluation(video_path+\"fastHighway_transferred_from_merge.mp4\", evaluate_type='state',\n",
        "                                                                                    iter_num=4, evaluate_episode_num=3600, use_saved_model=True)\n",
        "print('\\n\\nSum rewards in one EPISODE:', sum_rewards)"
      ],
      "metadata": {
        "id": "jCTxcz50j74J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_and_plot().show_video(directory=video_path, file_name=\"fastHighway_transferred_from_merge.mp4\")"
      ],
      "metadata": {
        "id": "Av_RN8RUmaMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Plotting**"
      ],
      "metadata": {
        "id": "VM8nZSopbY_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_reward_dqn_fastHighway = pd.read_excel(data_path_2 + 'Episodes_reward_run_5.xlsx')\n",
        "episodes_rewards_fastHighway = df_reward_dqn_fastHighway.T.to_numpy()\n",
        "df_reward_dqn_fastHighway_transferred = pd.read_excel(data_path_3 + 'Episodes_reward_run_5.xlsx')\n",
        "episodes_rewards_fastHighway_transferred = df_reward_dqn_fastHighway_transferred.T.to_numpy()\n",
        "\n",
        "fig, ax = plt.subplots(figsize = (12, 7.5))\n",
        "ax = show_and_plot().plot_training_result(episodes_rewards_fastHighway, ax, title='DQN',\n",
        "                                              label='fastHighway task with state', window=50, stride=10, alpha=0.3)\n",
        "ax = show_and_plot().plot_training_result(episodes_rewards_fastHighway_transferred, ax, title='DQN',\n",
        "                                              label='fastHighway transferred from Merge task with state', window=50, stride=10, alpha=0.3)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "fig.savefig(path_HW5+'Images/2-fastHighway_with_and_without_transfer_learning_training_on-state.png', dpi=300)"
      ],
      "metadata": {
        "id": "ukmBBVzXbY_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 4**"
      ],
      "metadata": {
        "id": "fxTkDmYGZmD5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## merge-v0 with CNN and observation"
      ],
      "metadata": {
        "id": "DfHFPQnPdc-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "# This part is for solving the 'error: No available video device' and 'error: x11 not avaliable'\n",
        "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
        "env1 = gym.make(env_name, render_mode='rgb_array')\n",
        "env1.reset()\n",
        "env1.render();\n",
        "os.environ['SDL_VIDEODRIVER'] = \"x11\"\n",
        "env2 = gym.make(env_name, render_mode='rgb_array')\n",
        "env2.reset()\n",
        "env2.render();\n",
        "################################################################################\n",
        "video_path = path_HW5 + 'Videos/'\n",
        "model_path = path_HW5 + 'Models/4_Models_CNN_merge/'\n",
        "data_path_4 = path_HW5 + 'Data_Average_Reward/4_Data_CNN_merge/'\n",
        "env_name = 'merge-v0'\n",
        "env = gym.make(env_name, render_mode='rgb_array')\n",
        "dqn_merge_cnn = DQN(env, env_name, model_path, data_path_4, network_type='CNN')"
      ],
      "metadata": {
        "id": "vIyamkmXdc-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "vS0j_fgrdc-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_reward_dqn_merge_cnn = dqn_merge_cnn.train_with_observation(n_iteration=[1, 2, 3, 4, 5], n_training_episodes=3600, max_step=10000)"
      ],
      "metadata": {
        "id": "HrZSqZnOdc-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating"
      ],
      "metadata": {
        "id": "A2WyoVzedc-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dqn_merge_cnn.env.config['policy_frequency'] = 10\n",
        "sum_rewards = dqn_merge_cnn.evaluation(video_path+\"merge_observation_CNN.mp4\", evaluate_type='observation',\n",
        "                                                                                  iter_num=2, evaluate_episode_num=2000, use_saved_model=True)\n",
        "print('\\n\\nSum rewards in one EPISODE:', sum_rewards)"
      ],
      "metadata": {
        "id": "HMJveqYgr84E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_and_plot().show_video(directory=video_path, file_name=\"merge_observation_CNN.mp4\")"
      ],
      "metadata": {
        "id": "-9M8S_N5dc-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Plotting**"
      ],
      "metadata": {
        "id": "JA6dZtrxdc-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_reward_dqn_merge = pd.read_excel(data_path_1 + 'Episodes_reward_run_5.xlsx')\n",
        "episodes_rewards_merge = df_reward_dqn_merge.T.to_numpy()\n",
        "df_reward_dqn_merge_cnn = pd.read_excel(data_path_4 + 'Episodes_reward_run_5.xlsx')\n",
        "episodes_rewards_merge_cnn = df_reward_dqn_merge_cnn.T.to_numpy()\n",
        "\n",
        "fig, ax = plt.subplots(figsize = (12, 7.5))\n",
        "ax = show_and_plot().plot_training_result(episodes_rewards_merge, ax, title='DQN', label='Merge task with state',\n",
        "                                                                                                          window=50, stride=10, alpha=0.3)\n",
        "ax = show_and_plot().plot_training_result(episodes_rewards_merge_cnn, ax, title='DQN', label='Merge task with CNN network and observation',\n",
        "                                                                                                          window=50, stride=10, alpha=0.3)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "fig.savefig(path_HW5+'Images/3-merge_cnn_observation_merge_linear_state.png', dpi=300)"
      ],
      "metadata": {
        "id": "r_ujsbY-dc-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Checking statistical significance**"
      ],
      "metadata": {
        "id": "t2HkaQT0Dvrj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "t_test = ttest_ind(np.mean(episodes_rewards_merge, axis=0), np.mean(episodes_rewards_merge_cnn, axis=0))\n",
        "print(\"P-value between two Average rewards is:\", t_test.pvalue)"
      ],
      "metadata": {
        "id": "CmVznXhroXIR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}